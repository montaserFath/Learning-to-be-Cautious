{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xr1RCs9qdhco"
   },
   "source": [
    "# Play k-of-n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2egOflEdhcs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as transforms\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\" # or \"E-MNIST\", \"MNIST-Fashion\" \n",
    "method = \"TEST\" # or \"test\" if you want to play k-of-n a sample by sample in dataset, \"TEST\" play k-of-n for full datatset\n",
    "ks = [10, 5, 1] # k values\n",
    "ns = [10, 10, 10] # n values\n",
    "n_itr = 100 # number of itration for k-of-n game\n",
    "n_models = 2000  # number of models in Enamble\n",
    "n_runs = 10 # how many times you want to repeat each k-of-n policy\n",
    "batch_size = 1024 # batch size for k-of-n\n",
    "gpu = 0 # number of gpus -1 if cpu\n",
    "replacment = False # True if you want to sample reward functions from reward distribution with replacment\n",
    "models_dir = \"\" # diroctory path where deep models have been saved\n",
    "output_policies_dir = \" \" # diroctory path where you want to save k-of-n policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check hyper-parameters work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len (ks) != len (ns):\n",
    "    raise ValueError (\"ks is not equal ns\")\n",
    "    \n",
    "if not replacment:\n",
    "    if (max(ns)* n_itr) > n_models:\n",
    "         raise ValueError (\"without replacment requreies more models\")\n",
    "\n",
    "for k in range (len(ks)):\n",
    "    if ns[k] < ks[k]:\n",
    "        raise ValueError (\"n value={} should be greater than or equal to k value={}\".format(ns[k],  ks[k]))\n",
    "\n",
    "if gpu >=0:\n",
    "    if torch.cuda.device_count() >= gpu:\n",
    "        raise ValueError (\"GPU/s is/are not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gh17GqWZdhc8"
   },
   "source": [
    "## datasets and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWzGtDtCdhdM"
   },
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "if dataset = \"MNIST\":\n",
    "    mnist_test = torchvision.datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(mnist_test,  shuffle=False)\n",
    "    testing_set_x = torch.zeros((len(testloader) , 1, 28, 28))\n",
    "    testing_set_y = torch.zeros(len(testloader))\n",
    "    for i, data in enumerate (testloader):\n",
    "        img, label = data\n",
    "        testing_set_x[i] = img.view(1,28,28)\n",
    "        testing_set_y[i] = label\n",
    "\n",
    "elif dataset = \"E-MNIST\":\n",
    "    emnist_test  = torchvision.datasets.EMNIST(root=\"datasets\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True, split=\"letters\")\n",
    "    testing_set_x = torch.zeros((len(emnist_test), 1, 28, 28))\n",
    "    for i in  range (len(emnist_test)):\n",
    "        testing_set_x[i]  = torch.transpose(emnist_test[i][0], 1,-1).view(1, 28,28)\n",
    "        \n",
    "elif dataset = \"MNIST-Fashion\":\n",
    "    fashion_mnist_test = torchvision.datasets.FashionMNIST(root=\"datasets\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "    testing_set_x = torch.zeros((len(fashion_mnist_test), 1, 28, 28), device=\"cpu\")\n",
    "    for i in  range (len(fashion_mnist_test)):\n",
    "        testing_set_x[i]  = fashion_mnist_test[i][0].view(1, 28,28)\n",
    "        \n",
    "else:\n",
    "    raise ValueError(\"dataset shoud be MNIST, E-MNIST or Fashion-MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reward sampling and expected value function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rewards_from_ensemble (method, n_samples, states, actions, models_number, models_dir, batch_size):\n",
    "    '''\n",
    "    Sample reward functions from deep ensamble and calculate the expected value of the policy given current given\n",
    "    Args:\n",
    "    method: (str) \"test\" if you want to play k-of-n a sample by sample in dataset, \"TEST\" play k-of-n for full datatset\n",
    "    n_samples: (int) number of samples \n",
    "    states: (Tesnsor) states which the policy will evaluate on it\n",
    "    actions:(Tesnsor) policy\n",
    "    models_number:(list or 1d numpy array) models number you want to sample from it\n",
    "    models_dir: (str) diroctory path where trained models saved\n",
    "    batch_size: (int) batch size\n",
    "    return:\n",
    "    expected_rewards:(Tensor shape (n_samples, ) ) expected value for each sample\n",
    "    n_rs: (Tensor shape (n_samples, states_size, action_size) ) reward for each sample and each state\n",
    "    ms: (1d numpy array) sampled models number\n",
    "    '''\n",
    "    ms = np.random.choice(models_number, n_samples)\n",
    "    n_rs = torch.zeros((n_samples, states.shape[0], actions.shape[1]))\n",
    "\n",
    "    if method == \"TEST\":\n",
    "        expected_rewards = torch.zeros((n_samples,))\n",
    "    else:\n",
    "        expected_rewards = torch.zeros((n_samples, states.shape[0]))\n",
    "    \n",
    "    for s in range (n_samples):\n",
    "        model = torch.load(models_dir+\"/ensemble_model_{}\".format(ms[s]))\n",
    "        for batch in range (0, states.shape[0] , batch_size):\n",
    "            n_rs[s, batch : batch+batch_size] = (model(states[batch : batch+batch_size].cuda())).detach().cpu()\n",
    "        if method == \"TEST\":\n",
    "            estimated_rewards[s]  = torch.sum(n_rs[s]*actions).item()\n",
    "        else:\n",
    "            estimated_rewards[s]  = torch.sum(n_rs[s]*actions, 1)\n",
    "\n",
    "    return estimated_rewards, n_rs, ms, np.setdiff1d(models_number, ms)\n",
    "def sort_and_k_least (a, k):\n",
    "    '''\n",
    "    sort a tensor from small value to big value and select lowest k values\n",
    "    Inputs:\n",
    "    a: (tesnor) tensor we want to sort it\n",
    "    k: (int) size of lowest values\n",
    "    return:\n",
    "    numpy array (shape (k, ))\n",
    "    '''\n",
    "    _, index = torch.sort(a,  descending=False)\n",
    "    return index[:k].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_of_n (ks, ns, n_runs, n_itr, method, state, n_models,batch_size, models_dir, output_policies_dir):\n",
    "    '''\n",
    "    Run k-of-n\n",
    "    Inputs:\n",
    "    ks: (list) k values\n",
    "    ns: (list) n values\n",
    "    n_runs: (int) how many times you want to repeat each k-of-n policy\n",
    "    n_itr: (int) number of itration for k-of-n game\n",
    "    method: (str) \"test\" if you want to play k-of-n a sample by sample in dataset, \"TEST\" play k-of-n for full datatset\n",
    "    state: (tensor) states which the policy will evaluate on it dataset MNIST, E-MNIST or Fashion\n",
    "    n_models: (int) number of models in Enamble\n",
    "    batch_size: (int) batch size\n",
    "    models_dir: (str) diroctory path where deep models have been saved\n",
    "    output_policies_dir: (str) diroctory path where you want to save k-of-n policies\n",
    "    '''\n",
    "    for i in range (len(ks)):\n",
    "        k = ks[i]\n",
    "        n = ns[i]\n",
    "        for run in range (n_runs):\n",
    "            expexted_value = np.zeros(n_itr)\n",
    "            actions = torch.softmax(torch.ones((state.shape[0], 10), device=\"cpu\"), dim=1 )\n",
    "            total_regret = torch.zeros((actions.shape[0] , actions.shape[1]), device=\"cpu\")\n",
    "            models_numbers = np.arange(0, n_models, 1,dtype=np.int)\n",
    "            for itr in range (n_itr):\n",
    "                n_estimated_rewards, n_rs, mss , models_numbers = sample_rewards_from_ensemble (method, n, itr,  state, actions, models_numbers, models_dir, batch_size)\n",
    "                if ==\"TEST\":\n",
    "                    k_index = sort_and_k_least(n_estimated_rewards, k)\n",
    "                    expexted_value[itr] = n_estimated_rewards[k_index].sum()\n",
    "                    mean_rs  = torch.mean(n_rs[k_index], 0)\n",
    "                    P_t = mean_rs - torch.mm(torch.sum(actions*mean_rs, 1, dtype=torch.float).view(-1, 1), torch.ones((1, actions.shape[1]), dtype=torch.float))\n",
    "                    total_regret += P_t\n",
    "\n",
    "                else:\n",
    "                    for s in range (state.shape[0]):\n",
    "                        k_index = sort_and_k_least(n_estimated_rewards[:, s], k)\n",
    "                        expexted_value[itr] += n_estimated_rewards[k_index, s].sum()\n",
    "                        a = n_rs[:, s]\n",
    "                        mean_rs  = torch.mean(a[k_index], 0)\n",
    "                        P_t = mean_rs - torch.sum(mean_rs*actions[s])*torch.ones((1, actions.shape[1]))\n",
    "                        total_regret[s] += P_t.view(-1)\n",
    "\n",
    "                    actions = relu(total_regret)/(torch.ones_like(total_regret)*torch.sum(relu(total_regret), 1).view(-1, 1))\n",
    "                    print(\"run no {} itration number {}\".format(run, itr))\n",
    "                    display.clear_output(wait=True)  \n",
    "                np.save(output_policies_dir+\"/run_{}_mnist_actions_{}-of-{}_n_itr_{}\".format(run, k,n, n_itr), actions.numpy())\n",
    "                np.save(output_policies_dir+\"/run_{}_expected_value_mnist_{}-of-{}_n_itr_{}\".format(run, k,n, n_itr), expexted_value)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "k-of-n-risk-reward.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
